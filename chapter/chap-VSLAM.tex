\chapter{同时定位与建图技术(SLAM)的介绍}
\section{增强现实技术}
增强现实技术(\textbf{augmented reality})是一种将真实世界信息和虚拟世界信息“无缝”集成的新技术，是把原本在现实世界的一定时间空间范围内很难体验到的实体信息(视觉信息,声音,味道,触觉等),通过电脑等科学技术，模拟仿真后再叠加，将虚拟的信息应用到真实世界，被人类感官所感知，从而达到超越现实的感官体验。比起传统方式来说，它更加的直观，更加的高效，因此也有着更加广阔的应用前景。近年来，增强现实技术已在军事，生活，游戏等众多领域运营并取得了成功。例如，宜家家居公司已经开发了一个APP使得用户可以使用智能手机观察不同的家具在自己房间的摆放效果；而任天堂公司也开发了Pokemon-Go 游戏，使得玩家可以通过智能手机在现实世界里发现精灵。

\section{增强现实的定位方式}
增强现实需要实时定位设备在环境中的方位，定位的方案虽然有许多种，但多数方案都存在局限或者代价太高难以普及，例如GPS无法在室内及遮挡严重的环境里使用，且精度较低，而基于无线信号的定位方案则需要事先布置场景。基于视觉的同时定位与地图构建技术（\textit{visual simultaneous localization and mapping} \textbf{V-SLAM}）以其成本低廉、小场景精度较高、无需预先布置场景等优势成为比较常采用的定位方案。

\section{V-SLAM技术}
V-SLAM 技术指的是使用图像作为外部信息的唯一来源，来定位一个机器人、一辆车或者一个移动的相机在整个场景中的位置，同时，重建环境的三维结构。


\subsection{V-SLAM的基本原理}

\begin{figure}[!htbp]
\centering
\includegraphics[width=12cm]{geometry.png}
\caption{多视图集合的基本原理示意图}
\label{fig:MVGeometry}
\end{figure}
V-SLAM技术根据拍摄的视频、图像信息推断摄像头在环境的方位，同时构建环境地图，其原理为多视图几何原理(\textbf{Multiple view geometry theroy}\cite{Hartley2004}) V-SLAM的目标为同时恢复出每帧图像对应的相机运动参数$C_1$,$C_2$ … $C_m$以及场景三维结构$X_1$,$X_2$ … $X_n$,每个相机运动参数$C_i$包含了相机的位置和朝向信息，通常表达为一个 3$\times$3的旋转矩阵$R_i$和一个三维位置变量 $p_i$。$R_i$与$p_i$将一个世界坐标系下的三维点$X_j$变换至$C_i$的局部坐标系
\begin{equation}\label{eq:1.1}
{(X_{ij},Y_{ij},Z_{ij})}^\mathrm{T}=R_{i}(X_{j}-p_{i})
\end{equation}
进而投影至图像中
\begin{equation}\label{eq:1.2}
h_{ij}={(f_{x}X_{ij}/Z_{ij}+c_{x}.f_{y}Y_{ij}/Z_{ij}+c_{y})}^\mathrm{T}
\end{equation}
其中，$f_x$,$f_y$分别为沿图像$x$,$y$轴的图像焦距,$(c_x,c_y)$为镜头光心在图像中的位置，通常假设这些参数已实现标定且保持不变，由式
\autoref{eq:1.1} \autoref{eq:1.2}，三维点在图像中的投影位置$h_{ij}$可表示为一个关于$C_i$和$X_j$的函数，记为
\begin{equation}\label{eq:1.3}
h_{ij}=h(C_i,X_j)
\end{equation}
V-SLAM算法需要将、对不同图像中对应于相同场景的图像点进行匹配，而这个过程是通过求解如下目标函数
\begin{equation}\label{eq:1.4}
\mathop{\arg\min}_{C_1,…C_m,X_1,…X_n} \sum_{i=1}^{m} {\sum_{j=1}^{n} \ \| {h} (C_i,X_j)-\tilde{x_{ij}}\|_{\Sigma_{ij}}}
\end{equation}
得到一组最优的$C_1$，$C_2$…$C_m$,$X_1$,$X_2$ … $X_n$，使得所有$X_j$在$C_i$图像中的投影位置$h_{ij}$与观测到的图像点位置$x_{ij}$尽可能靠近，这里假设图像观测点符合高斯分布$x_{ij}$ $\sim$ $\mathrm{\textit{N}}$ ($\tilde{x_{ij}}$,$\Sigma_{ij}$)，$\|$ $e$ $\|$ =$e^{\mathrm{T}}$ $\Sigma^{-1}$ $e$
求解目标函数\autoref{eq:1.4}的过程也成为集束调整（\textbf{bundle adjustment,BA}）,该最优化问题可利用线性方程的稀疏结构高效求解。
\subsection{主流的单目V-SLAM方法}
由于现阶段大多数AR产品都以智能手机以及平板电脑作为载体，而智能手机的摄像头大多以单目为主，双目、三目摄像头甚至深度摄像头都未得到普及，因此本文主要讨论基于单目视觉的同时定位与地图构建方法。

目前，主流的V-SLAM方法主要为：基于滤波器、基于关键帧BA和基于直接跟踪，我们先来看看这三种方法。比较并分析其优劣，而后详细介绍基于关键帧BA的V-SLAM方法。

基于滤波器的V-SLAM的方法将系统每一时刻的状态\textit{t}用一个高斯概率模型表达，$x_t$$\sim$ $\mathrm{\textit{N}}$ ($\tilde{x_{t}}$,$\mathrm{\textit{P}}_{ij}$),其中$\tilde{x}_t$为当前时刻系统状态估计值，$P_t$为该估计值误差的协方差矩阵，系统状态由滤波器不断更新，其中，比较具有代表性的基于滤波器的V-SLAM方法有MonoSLAM以及MSCKF。

而基于关键帧BA的V-SLAM方法\cite{TriggsB.HartleyR.I.FitzgibbonA.W.2000}是近年来最流行的方法之一，他的主要思想是将相机跟踪（Tracking）和地图构建（Mapping）作为两个独立的任务在两个线程并行执行，而Mapping线程仅维护视频流中抽取的关键帧。PTAM是最著名的基于关键帧BA的方法之一，也是我们介绍的重点。

基于直接跟踪的V-SLAM方法则是直接通过比较像素颜色来求解相机运动，具有代表性的算法有DTAM以及LSD-SLAM。
\subsection{不同V-SLAM方法的比较}
比较三种方法\cite{刘浩敏2016基于单目视觉的同时定位与地图构建方法综述}，基于直接跟踪的V-SLAM方法对光照变化和动态干扰较为敏感，所以精度会若于基于关键帧BA的V-SLAM方法，而基于滤波器的V-SLAM由于容易在变量未精确的时候就进行消元，误差不断累积，因此精度最差。

而在定位效率与场景尺度方面，基于滤波器的V-SLAM方法计算复杂度较高，只适用于几百个点的小场景，因此定位效率和场景尺度较低。基于关键帧的V-SLAM方法例如PTAM，它只优化当前帧方位，因此具有最高的定位效率，但场景尺度受到BA的效率限制因此也不高。而基于直接跟踪的V-SLAM方法定位效率取决于图像，分辨率，且采用更高效的方位图优化替代全局BA，因此具有最高的场景尺度限制。

在近似纯旋转扩展鲁棒性上，由于在实际使用中，相机会近似纯旋转的方式转向拍摄旁边的场景内容，因此基于滤波器的V-SLAM方法每帧同时优化三维点和相机方位，因此对近似纯旋转具有很好的鲁棒性。基于关键帧BA的V-SLAM方法在近似纯旋转扩展场景时容易因视差不够无法三角化新的三维点，从而无法加入新的关键帧导致关键帧丢失，鲁棒性较差。基于直接跟踪的V-SLAM方法的鲁棒性则取决于后台场景地图的扩展及优化的效率，因此鲁棒性较好。

对于场景变化的鲁棒性来说，无论哪种V-SLAM方法，都假设场景是静止不变的，如果场景变化很大都会导致跟踪的失败。因此对于场景变化的鲁棒性都不佳。在第四节中，我们会专门讨论运动情况下的V-SLAM方法。

总的来说，基于关键帧的V-SLAM方法具有更好的效果。

